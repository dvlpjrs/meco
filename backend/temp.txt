{"main_function": {"id": "f29fc928-f651-4204-8735-c1b29678dae0", "name": "calculate_prompt_cost", "args": [{"name": "prompt", "type": "Union[List[dict], str]"}, {"name": "model", "type": "str"}], "returns": "Decimal", "body": "def calculate_prompt_cost(prompt: Union[List[dict], str], model: str) -> Decimal:\n    \"\"\"\n    Calculate the prompt's cost in USD.\n\n    Args:\n        prompt (Union[List[dict], str]): List of message objects or single string prompt.\n        model (str): The model name.\n\n    Returns:\n        Decimal: The calculated cost in USD.\n\n    e.g.:\n    >>> prompt = [{ \"role\": \"user\", \"content\": \"Hello world\"},\n                  { \"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n    >>>calculate_prompt_cost(prompt, \"gpt-3.5-turbo\")\n    Decimal('0.0000300')\n    # or\n    >>> prompt = \"Hello world\"\n    >>> calculate_prompt_cost(prompt, \"gpt-3.5-turbo\")\n    Decimal('0.0000030')\n    \"\"\"\n    model = model.lower()\n    model = strip_ft_model_name(model)\n    if model not in TOKEN_COSTS:\n        raise KeyError(\n            f\"\"\"Model {model} is not implemented.\n            Double-check your spelling, or submit an issue/PR\"\"\"\n        )\n    if not isinstance(prompt, (list, str)):\n        raise TypeError(\n            f\"\"\"Prompt must be either a string or list of message objects.\n            it is {type(prompt)} instead.\n            \"\"\"\n        )\n    prompt_tokens = (\n        count_string_tokens(prompt, model)\n        if isinstance(prompt, str)\n        else count_message_tokens(prompt, model)\n    )\n\n    return calculate_cost_by_tokens(prompt_tokens, model, \"input\")", "filename": "./temp_dir/6714b1dcfacd89c0579a61e0/code/tokencost/tokencost/costs.py", "lineno": 173, "end_lineno": 213, "class": null}, "linked_sub_functions": [{"name": "strip_ft_model_name", "body": "def strip_ft_model_name(model: str) -> str:\n    \"\"\"\n    Finetuned models format: ft:gpt-3.5-turbo:my-org:custom_suffix:id\n    We only need the base model name to get cost info.\n    \"\"\"\n    if model.startswith(\"ft:gpt-3.5-turbo\"):\n        model = \"ft:gpt-3.5-turbo\"\n    return model", "args": [{"name": "model", "type": "str"}], "returns": "str"}, {"name": "count_message_tokens", "body": "def count_message_tokens(messages: List[Dict[str, str]], model: str) -> int:\n    \"\"\"\n    Return the total number of tokens in a prompt's messages.\n    Args:\n        messages (List[Dict[str, str]]): Message format for prompt requests. e.g.:\n            [{ \"role\": \"user\", \"content\": \"Hello world\"},\n             { \"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n        model (str): Name of LLM to choose encoding for.\n    Returns:\n        Total number of tokens in message.\n    \"\"\"\n    model = model.lower()\n    model = strip_ft_model_name(model)\n\n    if \"claude-\" in model:\n        \"\"\"\n        Note that this is only accurate for older models, e.g. `claude-2.1`. \n        For newer models this can only be used as a _very_ rough estimate, \n        instead you should rely on the `usage` property in the response for exact counts.\n        \"\"\"\n        prompt = \"\".join(message[\"content\"] for message in messages)\n        return count_string_tokens(prompt, model)\n\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        logger.warning(\"Model not found. Using cl100k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    if model in {\n        \"gpt-3.5-turbo-0613\",\n        \"gpt-3.5-turbo-16k-0613\",\n        \"gpt-4-0314\",\n        \"gpt-4-32k-0314\",\n        \"gpt-4-0613\",\n        \"gpt-4-32k-0613\",\n        \"gpt-4-turbo\",\n        \"gpt-4-turbo-2024-04-09\",\n        \"gpt-4o\",\n        \"gpt-4o-2024-05-13\",\n    }:\n        tokens_per_message = 3\n        tokens_per_name = 1\n    elif model == \"gpt-3.5-turbo-0301\":\n        # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n        tokens_per_message = 4\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    elif \"gpt-3.5-turbo\" in model:\n        logger.warning(\n            \"gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\"\n        )\n        return count_message_tokens(messages, model=\"gpt-3.5-turbo-0613\")\n    elif \"gpt-4o\" in model:\n        print(\n            \"Warning: gpt-4o may update over time. Returning num tokens assuming gpt-4o-2024-05-13.\")\n        return count_message_tokens(messages, model=\"gpt-4o-2024-05-13\")\n    elif \"gpt-4\" in model:\n        logger.warning(\n            \"gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\"\n        )\n        return count_message_tokens(messages, model=\"gpt-4-0613\")\n    else:\n        raise KeyError(\n            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\n            See https://github.com/openai/openai-python/blob/main/chatml.md for how messages are converted to tokens.\"\"\"\n        )\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n    return num_tokens", "args": [{"name": "messages", "type": "List[Dict[str, str]]"}, {"name": "model", "type": "str"}], "returns": "int"}, {"name": "count_string_tokens", "body": "def count_string_tokens(prompt: str, model: str) -> int:\n    \"\"\"\n    Returns the number of tokens in a (prompt or completion) text string.\n\n    Args:\n        prompt (str): The text string\n        model_name (str): The name of the encoding to use. (e.g., \"gpt-3.5-turbo\")\n\n    Returns:\n        int: The number of tokens in the text string.\n    \"\"\"\n    model = model.lower()\n\n    if \"/\" in model:\n        model = model.split(\"/\")[-1]\n\n    if \"claude-\" in model:\n        \"\"\"\n        Note that this is only accurate for older models, e.g. `claude-2.1`. \n        For newer models this can only be used as a _very_ rough estimate, \n        instead you should rely on the `usage` property in the response for exact counts.\n        \"\"\"\n        if \"claude-3\" in model:\n            logger.warning(\n                \"Warning: Claude-3 models are not yet supported. Returning num tokens assuming claude-2.1.\"\n            )\n        client = anthropic.Client()\n        token_count = client.count_tokens(prompt)\n        return token_count\n\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        logger.warning(\"Warning: model not found. Using cl100k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n\n    return len(encoding.encode(prompt))", "args": [{"name": "prompt", "type": "str"}, {"name": "model", "type": "str"}], "returns": "int"}, {"name": "calculate_cost_by_tokens", "body": "def calculate_cost_by_tokens(num_tokens: int, model: str, token_type: str) -> Decimal:\n    \"\"\"\n    Calculate the cost based on the number of tokens and the model.\n\n    Args:\n        num_tokens (int): The number of tokens.\n        model (str): The model name.\n        token_type (str): Type of token ('input' or 'output').\n\n    Returns:\n        Decimal: The calculated cost in USD.\n    \"\"\"\n    model = model.lower()\n    if model not in TOKEN_COSTS:\n        raise KeyError(\n            f\"\"\"Model {model} is not implemented.\n            Double-check your spelling, or submit an issue/PR\"\"\"\n        )\n\n    cost_per_token_key = (\n        \"input_cost_per_token\" if token_type == \"input\" else \"output_cost_per_token\"\n    )\n    cost_per_token = TOKEN_COSTS[model][cost_per_token_key]\n\n    return Decimal(str(cost_per_token)) * Decimal(num_tokens)", "args": [{"name": "num_tokens", "type": "int"}, {"name": "model", "type": "str"}, {"name": "token_type", "type": "str"}], "returns": "Decimal"}]}